# Здесь логистическая регрессия градиентным спуском без использования библиотек
import pandas as pd
import numpy as np
import math
from sklearn.metrics import roc_auc_score  # Метрика качества классификации.
# Сравнивает массив вероятностных результатов с массивом ответов обучающей выборки


data = pd.read_csv('data-logistic.csv', header=None)

w1 = 0  # Инициализация начальны весов
w2 = 0
C = 10  # Коэффициент регуляризации
k = 0.1  # Длина шага


# Это производная сигмоиды как я понял, используется для нахождения градиента
def logistic(y, x1, x2):
    return 1. - 1. / (1. + math.exp(-1. * y * (w1 * x1 + w2 * x2)))


for i in range(1000):  # Ограничение итераций обучения
    # k * C * w - регуляризация L2 (здесь без возведения в квадрат), не даёт весам уходить в бесконечность,
    # тем самым избавляет от переобучения в случае наличия линейно зависимых признаков, тут ускоряет обучение
    gradw1 = k * np.mean([row[0] * row[1] * logistic(row[0], row[1], row[2])  # Поправка w1
                          for numrow, row in data.iterrows()]) - k * C * w1

    # Вычисления можно было сделать в матричном виде, если исходные данные конвертировать из форматов pandas в numpy
    gradw2 = k * np.mean([row[0] * row[2] * logistic(row[0], row[1], row[2])  # Поправка w2
                          for numrow, row in data.iterrows()]) - k * C * w2
    # Если евклидово расстояние между новыми и старыми весами меньше 0.00001, то обучение завершено
    if abs(math.pow(math.pow(w1 + gradw1, 2) + math.pow(w2 + gradw2, 2), 0.5) -
           math.pow(math.pow(w1, 2) + math.pow(w2, 2), 0.5)) < 1e-5:
        print('iter breaked at step:', i)
        break
    w1 += gradw1
    w2 += gradw2

ytrue = data.filter(items=[0])  # Список ответов обучающей выборки
# Сигмоида -> Список ответов объектов обучающей выборки на обученных весах. Это вероятности на самом деле
yscore = [1. / (1. + math.exp(-1. * (w1 * row[1] + w2 * row[2]))) for numrow, row in data.iterrows()]

# Одна из метрик оценки качества обучения
res = roc_auc_score(y_true=ytrue, y_score=yscore)
print(res)  # 0.928 без регуляризации(С=0), 0.936 с регуляризацией(С=10)
# Начальные веса w1 и w2 здесь не влияют на результат и почти не влияют на скорость обучения
# Длина шага k и коэф регуляризации С влияют на скорость обучения
